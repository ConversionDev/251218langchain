"""
ğŸ˜ğŸ˜ chat_service_t.py ì„œë¹™ ê´€ë ¨ ì„œë¹„ìŠ¤

PEFT QLoRA ë°©ì‹ìœ¼ë¡œ ëŒ€í™”í•˜ê³  í•™ìŠµí•˜ëŠ” ê¸°ëŠ¥ í¬í•¨.

ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬, ìš”ì•½, í† í° ì ˆì•½ ì „ëµ ë“±.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

import torch
from datasets import Dataset
from peft import (
    LoraConfig,
    PeftModel,
    TaskType,
    get_peft_model,
    prepare_model_for_kbit_training,
)
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    DataCollatorForLanguageModeling,
    TrainingArguments,
)

try:
    from trl import SFTTrainer
except ImportError:
    from trl.trainer.sft_trainer import SFTTrainer


class ChatServiceQLoRA:
    """QLoRAë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ë° í•™ìŠµ ì„œë¹„ìŠ¤."""

    def __init__(
        self,
        model_name_or_path: str,
        output_dir: str = "./qlora_output",
        lora_r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.05,
        target_modules: Optional[List[str]] = None,
        device_map: str = "auto",
    ):
        """QLoRA ì±„íŒ… ì„œë¹„ìŠ¤ ì´ˆê¸°í™”.

        Args:
            model_name_or_path: ëª¨ë¸ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ
            output_dir: í•™ìŠµ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
            lora_r: LoRA rank
            lora_alpha: LoRA alpha
            lora_dropout: LoRA dropout
            target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ ëª©ë¡ (Noneì´ë©´ ìë™ ê°ì§€)
            device_map: ë””ë°”ì´ìŠ¤ ë§¤í•‘ ("auto", "cpu", "cuda" ë“±)
        """
        self.model_name_or_path = model_name_or_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # QLoRA ì„¤ì • (4-bit quantization)
        self.bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

        # LoRA ì„¤ì •
        if target_modules is None:
            # ì¼ë°˜ì ì¸ ëª¨ë¸ì˜ attention ëª¨ë“ˆ (Llama, Mistral ë“±)
            target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]

        self.lora_config = LoraConfig(
            r=lora_r,
            lora_alpha=lora_alpha,
            target_modules=target_modules,
            lora_dropout=lora_dropout,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer: Optional[AutoTokenizer] = None
        self.model: Optional[Any] = None
        self.peft_model: Optional[PeftModel] = None
        self.device_map = device_map

        # ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.chat_sessions: Dict[str, List[Dict[str, str]]] = {}

    def load_model(self) -> None:
        """ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ."""
        print(f"[INFO] ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name_or_path}")

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            self.model_name_or_path,
            trust_remote_code=True,
        )

        # pad_token ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id

        self.tokenizer = tokenizer

        # ëª¨ë¸ ë¡œë“œ (4-bit quantization)
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name_or_path,
            quantization_config=self.bnb_config,
            device_map=self.device_map,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
        )

        # PEFT ëª¨ë¸ ì¤€ë¹„
        model = prepare_model_for_kbit_training(model)

        # LoRA ì ìš©
        peft_model = get_peft_model(model, self.lora_config)
        peft_model.print_trainable_parameters()

        self.model = model
        self.peft_model = peft_model

        print("[OK] ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def load_peft_model(self, peft_model_path: str) -> None:
        """í•™ìŠµëœ PEFT ëª¨ë¸ ë¡œë“œ.

        Args:
            peft_model_path: PEFT ëª¨ë¸ ê²½ë¡œ
        """
        if self.model is None:
            raise RuntimeError("ë¨¼ì € load_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        print(f"[INFO] PEFT ëª¨ë¸ ë¡œë”© ì¤‘: {peft_model_path}")
        self.peft_model = PeftModel.from_pretrained(
            self.model, peft_model_path, device_map=self.device_map
        )
        print("[OK] PEFT ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

    def chat(
        self,
        message: str,
        session_id: str = "default",
        history: Optional[List[Dict[str, str]]] = None,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
    ) -> str:
        """ëŒ€í™” ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            session_id: ì„¸ì…˜ ID
            history: ëŒ€í™” ê¸°ë¡ (Noneì´ë©´ ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì‚¬ìš©)
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„
            top_p: nucleus sampling íŒŒë¼ë¯¸í„°

        Returns:
            ìƒì„±ëœ ì‘ë‹µ
        """
        if self.peft_model is None:
            raise RuntimeError("ë¨¼ì € load_model() ë˜ëŠ” load_peft_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        # ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
        if history is None:
            history = self.chat_sessions.get(session_id, [])

        # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self._format_chat_prompt(message, history)

        # í† í¬ë‚˜ì´ì§•
        if self.tokenizer is None:
            raise RuntimeError("í† í¬ë‚˜ì´ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        inputs = self.tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=2048
        ).to(self.peft_model.device)

        # ìƒì„±
        with torch.no_grad():
            outputs = self.peft_model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id
                if self.tokenizer.pad_token_id
                else self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )

        # ë””ì½”ë”©
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # ì‘ë‹µë§Œ ì¶”ì¶œ (í”„ë¡¬í”„íŠ¸ ì œì™¸)
        response = generated_text[len(prompt) :].strip()

        # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.chat_sessions[session_id] = history + [
            {"role": "user", "content": message},
            {"role": "assistant", "content": response},
        ]

        return response

    def _format_chat_prompt(self, message: str, history: List[Dict[str, str]]) -> str:
        """ëŒ€í™” í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±.

        Args:
            message: í˜„ì¬ ë©”ì‹œì§€
            history: ëŒ€í™” ê¸°ë¡

        Returns:
            í¬ë§·ëœ í”„ë¡¬í”„íŠ¸
        """
        prompt_parts = []

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        prompt_parts.append("ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")

        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        for msg in history:
            role = msg.get("role", "user")
            content = msg.get("content", "")
            if role == "user":
                prompt_parts.append(f"ì‚¬ìš©ì: {content}")
            elif role == "assistant":
                prompt_parts.append(f"ì–´ì‹œìŠ¤í„´íŠ¸: {content}")

        # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
        prompt_parts.append(f"ì‚¬ìš©ì: {message}")
        prompt_parts.append("ì–´ì‹œìŠ¤í„´íŠ¸:")

        return "\n".join(prompt_parts)

    def train(
        self,
        training_data: List[Dict[str, str]],
        output_dir: Optional[str] = None,
        num_epochs: int = 3,
        per_device_train_batch_size: int = 4,
        gradient_accumulation_steps: int = 4,
        learning_rate: float = 2e-4,
        warmup_steps: int = 100,
        logging_steps: int = 10,
        save_steps: int = 500,
        max_seq_length: int = 512,
    ) -> str:
        """QLoRA í•™ìŠµ ì‹¤í–‰.

        Args:
            training_data: í•™ìŠµ ë°ì´í„° ({"instruction": "...", "input": "...", "output": "..."} í˜•ì‹)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            num_epochs: ì—í­ ìˆ˜
            per_device_train_batch_size: ë°°ì¹˜ í¬ê¸°
            gradient_accumulation_steps: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…
            learning_rate: í•™ìŠµë¥ 
            warmup_steps: ì›Œë°ì—… ìŠ¤í…
            logging_steps: ë¡œê¹… ìŠ¤í…
            save_steps: ì €ì¥ ìŠ¤í…
            max_seq_length: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´

        Returns:
            í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
        """
        if self.peft_model is None:
            raise RuntimeError("ë¨¼ì € load_model()ì„ í˜¸ì¶œí•˜ì„¸ìš”.")

        if self.tokenizer is None:
            raise RuntimeError("í† í¬ë‚˜ì´ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        output_dir = output_dir or str(
            self.output_dir / f"checkpoint-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
        )

        # ë°ì´í„°ì…‹ ì¤€ë¹„
        def format_prompt(example):
            """í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…."""
            instruction = example.get("instruction", "")
            input_text = example.get("input", "")
            output = example.get("output", "")

            if input_text:
                prompt = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
            else:
                prompt = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"

            return {"text": prompt}

        dataset = Dataset.from_list(training_data)
        dataset = dataset.map(format_prompt)

        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            save_total_limit=3,
            fp16=False,  # QLoRAëŠ” bfloat16 ì‚¬ìš©
            bf16=True,
            optim="paged_adamw_8bit",
            lr_scheduler_type="cosine",
            report_to="none",
        )

        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer, mlm=False
        )

        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer_kwargs: Dict[str, Any] = {
            "model": self.peft_model,
            "train_dataset": dataset,
            "peft_config": self.lora_config,
            "tokenizer": self.tokenizer,
            "args": training_args,
            "data_collator": data_collator,
            "max_seq_length": max_seq_length,
        }

        # packing íŒŒë¼ë¯¸í„°ëŠ” ë²„ì „ì— ë”°ë¼ ì„ íƒì 
        try:
            trainer = SFTTrainer(**trainer_kwargs, packing=False)  # type: ignore
        except TypeError:
            # packing íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê²½ìš°
            trainer_kwargs.pop("packing", None)
            trainer = SFTTrainer(**trainer_kwargs)  # type: ignore

        # í•™ìŠµ ì‹¤í–‰
        print("[INFO] í•™ìŠµ ì‹œì‘...")
        trainer.train()
        print("[OK] í•™ìŠµ ì™„ë£Œ")

        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)

        print(f"[OK] ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}")
        return output_dir

    def train_from_chat_history(
        self,
        session_ids: Optional[List[str]] = None,
        output_dir: Optional[str] = None,
        **train_kwargs,
    ) -> str:
        """ì±„íŒ… íˆìŠ¤í† ë¦¬ë¡œë¶€í„° í•™ìŠµ ë°ì´í„° ìƒì„± ë° í•™ìŠµ.

        Args:
            session_ids: í•™ìŠµí•  ì„¸ì…˜ ID ëª©ë¡ (Noneì´ë©´ ëª¨ë“  ì„¸ì…˜)
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            **train_kwargs: train() ë©”ì„œë“œì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì

        Returns:
            í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
        """
        # í•™ìŠµ ë°ì´í„° ìƒì„±
        training_data = []

        if session_ids is None:
            session_ids = list(self.chat_sessions.keys())

        for session_id in session_ids:
            history = self.chat_sessions.get(session_id, [])
            if len(history) < 2:
                continue

            # ëŒ€í™” ìŒìœ¼ë¡œ ë³€í™˜
            for i in range(0, len(history) - 1, 2):
                if i + 1 < len(history):
                    user_msg = history[i].get("content", "")
                    assistant_msg = history[i + 1].get("content", "")

                    training_data.append(
                        {
                            "instruction": "ë‹¤ìŒ ëŒ€í™”ì— ì‘ë‹µí•˜ì„¸ìš”.",
                            "input": user_msg,
                            "output": assistant_msg,
                        }
                    )

        if not training_data:
            raise ValueError("í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        print(f"[INFO] {len(training_data)}ê°œì˜ í•™ìŠµ ìƒ˜í”Œ ìƒì„±ë¨")

        # í•™ìŠµ ì‹¤í–‰
        return self.train(training_data, output_dir=output_dir, **train_kwargs)

    def save_session(self, session_id: str, file_path: str) -> None:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì €ì¥.

        Args:
            session_id: ì„¸ì…˜ ID
            file_path: ì €ì¥ ê²½ë¡œ
        """
        history = self.chat_sessions.get(session_id, [])
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
        print(f"[OK] ì„¸ì…˜ ì €ì¥ ì™„ë£Œ: {file_path}")

    def load_session(self, session_id: str, file_path: str) -> None:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ë¡œë“œ.

        Args:
            session_id: ì„¸ì…˜ ID
            file_path: ë¡œë“œ ê²½ë¡œ
        """
        with open(file_path, "r", encoding="utf-8") as f:
            history = json.load(f)
        self.chat_sessions[session_id] = history
        print(f"[OK] ì„¸ì…˜ ë¡œë“œ ì™„ë£Œ: {file_path}")

    def clear_session(self, session_id: str) -> None:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ì‚­ì œ.

        Args:
            session_id: ì„¸ì…˜ ID
        """
        if session_id in self.chat_sessions:
            del self.chat_sessions[session_id]
            print(f"[OK] ì„¸ì…˜ ì‚­ì œ ì™„ë£Œ: {session_id}")

    def get_session_history(self, session_id: str) -> List[Dict[str, str]]:
        """ì„¸ì…˜ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°.

        Args:
            session_id: ì„¸ì…˜ ID

        Returns:
            ëŒ€í™” ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
        """
        return self.chat_sessions.get(session_id, [])
